{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1ekdXl50skO_ENeGX1Apk_slg8TdOT-2g","authorship_tag":"ABX9TyO1afsdoX/Y64I2BFLmXFx9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"z-2jDZ0jxWTW","executionInfo":{"status":"ok","timestamp":1760714914220,"user_tz":-120,"elapsed":13377,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}}},"outputs":[],"source":["### ========= SIAMESE NETWORK CODE FOR SUBJECT MATCHING (MNIT Dataset) =======\n","## Note. This run on GPU\n","\n","\n","### --------- IMPORT PACKAGES\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","#data handling\n","import numpy as np\n","import random\n","#plotting\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","#datasets\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import MNIST\n","from torch import optim\n","from torch.utils.data import random_split\n","#ROC AUC curve\n","from sklearn.metrics import roc_curve, auc\n","import torch.nn.functional as F\n","#image loading\n","from google.colab import files\n","from PIL import Image, ImageOps\n","import io\n","#save on drive\n","from google.colab import drive\n","\n","\n","### --------- CLASSES DEFINTIONS\n","\n","## --- PAIRED IMAGES DATASET BUILDING\n","class SiameseData(Dataset):\n","    ''' Define a custom class (dataset + transform) to build a Siamese (i.e paired imaged) datatset\n","    starting from a given dataset and a given transform.\n","    Returns img1, img2 and the matching label\n","    '''\n","    #stores the dataset and optional transform for later use\n","    def __init__(self, data, transform=None):\n","        # Save the dataset (e.g., training or validation set) as an instance variable\n","        self.data = data\n","        # Save the transform as an instance variable\n","        self.transform = transform\n","\n","    def __len__(self):\n","        # Return the number of samples in the dataset\n","        return len(self.data)\n","\n","    #returns 1 sample (pair of imgs) given an index\n","    def __getitem__(self,index):\n","        #select img1 and its label\n","        img1, label1 = self.data[index]\n","\n","        #randomly decide if picking a matching label (same class = 1) or different labels (same class = 0) pair\n","        same_class = random.randint(0, 1)\n","\n","        # the pair MUST have a matching label (same class)\n","        if same_class == 1:\n","            #initilize label2 (with a value not included in MNIST (goes from 0-9))\n","            label2 = -1\n","            #loop to keep picking until a label match is found (stops when label2 == label1)\n","            while label2 != label1:\n","                #randomly picks and img and its label to compare with label1\n","                img2, label2 = random.choice(self.data)\n","\n","        #the pair MUST NOT have a matching label (different classes)\n","        else:\n","            #initilize label 2 = label1\n","            label2 = label1\n","            #loop to keep picking until a different label pic is found (stops when label2 != label1)\n","            while label2 == label1:\n","                #randomly picks and img and its label to compare with label1\n","                img2, label2 = random.choice(self.data)\n","\n","        # once a an img2 (either = or different) has been found transform both the imgs\n","        if self.transform: #(if a transform was defined)\n","            #apply transform to img1\n","            img1 = self.transform(img1)\n","            #apply transform to img2\n","            img2 = self.transform(img2)\n","\n","        #define the pair_label (1 if labels differ, else 0 (base case))\n","        pair_label = torch.tensor([(label1 != label2)], dtype = torch.float32)\n","        #check if labels are different, if are the output is True -> pytorch converts to 1\n","\n","        #return imgs pair and their label\n","        return img1, img2, pair_label\n","\n","\n","## --- CONTRASTIVE LOSS FUNCTION (custom loss function class for contrastive learning)\n","class ContrastLoss(torch.nn.Module):\n","    # Initialize the class, setting a 'margin' (minimum distance between embeddings of different classes)\n","    def __init__(self, margin = 2.0):\n","        # Inherit from PyTorch's base class for all neural network modules\n","        super(ContrastLoss, self).__init__()\n","        # Save the margin as an attribute of the loss function\n","        self.margin = margin\n","\n","    # Define how the loss is computed for each batch\n","    def forward(self, output1, output2, y):\n","        # Compute the Euclidean distance  (between the two outputs)\n","        euclid_dist = F.pairwise_distance(output1, output2, keepdim = True)\n","\n","        #similar pairs (y = 0): minimize the squared distance between embeddings\n","        same_class_loss = (1 - y) * (euclid_dist ** 2)\n","\n","        # different pairs (y = 1): penalize only if the distance is smaller than the margin\n","        diff_class_loss = (y) * (torch.clamp(self.margin - euclid_dist, min = 0.0) ** 2)\n","        # Note. this encourages diff pairs to be at least 'margin' apart\n","\n","        # take the mean of losses\n","        return torch.mean(same_class_loss + diff_class_loss)\n","\n","\n","## --- TUNING CONVOLUTIONAL NEURAL NETWORK (adding BATCH)\n","class SiameseNetworkBatch(nn.Module):\n","    ''' Siamese Network for tuning. Configuration is equal to the SiameseNetwork except for the addition of\n","    Batch Normalization in the convolutional layers.\n","    '''\n","    #initialize the base nn.module class\n","    def __init__(self):\n","        super(SiameseNetworkBatch, self).__init__()\n","\n","        # (3) convolutional layers to extract features from img\n","        self.cnn = nn.Sequential(\n","            #1st layer\n","            nn.Conv2d(1, 64, kernel_size = 5, stride = 1, padding = 2), #1 input channel -> 64 output channels\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace = True), #RELU activation function\n","            nn.MaxPool2d(2, stride = 2), #max pooling downsampling\n","\n","            #2nd layer\n","            nn.Conv2d(64, 128, kernel_size = 5, stride = 1, padding = 2), #64 input channels -> 128 outpts channels\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","\n","            #3rd layer (smaller filter size, same pixel moving each step, smaller border )\n","            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1), #128 input channels -> 256 outpts channels\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2)\n","        )\n","\n","        # Fully connected layers to convert CNN features into embeddings\n","        self.fc = nn.Sequential(\n","            #flats the CNN output and maps it to 1024 features\n","            nn.Linear(256 * 3 * 3, 1024),\n","            #apply ReLU activation\n","            nn.ReLU(inplace = True),\n","\n","            #reduces dimensionality (to 256)\n","            nn.Linear(1024, 256),\n","            #activation function\n","            nn.ReLU(inplace = True),\n","            #final embedding to 2, used to compute distances\n","            nn.Linear(256,2)\n","        )\n","\n","    #compute the embedding for 1img\n","    def forward_once(self, x):\n","        #pass the img to the cnn to extract features\n","        output = self.cnn(x)\n","        #flat the cnn output into a 1D vector x img\n","        output = output.view(output.size()[0], -1)\n","        #pass through fully connected layers to get the embedding\n","        output = self.fc(output)\n","        #get the output\n","        return output\n","\n","    #compute the embedding for a imgs PAIR (used to measure similarity)\n","    def forward(self, input1, input2):\n","        #get embedding on the 1st img\n","        output1 = self.forward_once(input1)\n","        #get embedding on the 2nd img\n","        output2 = self.forward_once(input2)\n","        #get embedding for both imgs\n","        return output1, output2\n","\n","\n","## --- TUNING EVALUATION FUNCTION\n","def evaluate(model, dataloader_val, criterion):\n","    \"\"\"\n","    Evaluate a trained Siamese network on the validation set.\n","    Returns the average validation loss over all batches.\n","    \"\"\"\n","    model.eval()\n","    val_total_loss = 0.0\n","\n","    with torch.no_grad():\n","        for img1, img2, pair_label in dataloader_val:\n","            img1, img2, pair_label = img1.cuda(), img2.cuda(), pair_label.cuda()\n","\n","            # Forward pass\n","            output1, output2 = model(img1, img2)\n","            val_loss_contrast = criterion(output1, output2, pair_label)\n","\n","            val_total_loss += val_loss_contrast.item()\n","\n","    avg_loss = val_total_loss / len(dataloader_val)\n","    return avg_loss\n","\n","\n","## --- SIMILARITY SCORE\n","def compute_similarity(output1, output2):\n","    # Compute Euclidean distance\n","    euclidean_distance = F.pairwise_distance(output1, output2)\n","    # Transform into similarity score in [0,1]\n","    similarity_score = torch.exp(-euclidean_distance)\n","    #return the score\n","    return similarity_score"]},{"cell_type":"code","source":["#### =========== DATA & MODEL LOADING & DEFINITION\n","\n","## --- DATA LOADING\n","#load the testing set\n","data_tt = MNIST(root = './data', train = False, download = True)\n","#define transformation pipeline convertin MNIST images from PIL to tensors\n","transform = transforms.Compose([transforms.ToTensor()])\n","\n","\n","## --- DATA TRANSFORMATION & PAIRING\n","# set up the various classes (original dataset + transform)\n","#testing\n","siamese_tt = SiameseData(data_tt, transform)\n","\n","\n","## --- DATALOADER, MODEL, OPTIMIZER AND LOSS FUNCTION DEFINITION\n","# contrastive loss function\n","crit = ContrastLoss()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umbRkptJxsPS","executionInfo":{"status":"ok","timestamp":1760714918316,"user_tz":-120,"elapsed":4087,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}},"outputId":"5cfc2543-5772-4112-d7c6-2cf9a3781441"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 12.6MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 338kB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 3.19MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 3.87MB/s]\n"]}]},{"cell_type":"code","source":["#### =========== MODEL LOADING & DEFINITION\n","tun_model = SiameseNetworkBatch().cuda()\n","#access to drive\n","drive.mount('/content/drive')\n","#load the already tuned model\n","tun_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Siamese_model.pt\"))\n","print(\"✅ Tuned model successfully loaded from 'Siamese_model.pt'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fPClBDAzZFI","executionInfo":{"status":"ok","timestamp":1760714931213,"user_tz":-120,"elapsed":12895,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}},"outputId":"f6b07bce-28cf-41cd-df7a-1a6d891d3cd5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Tuned model successfully loaded from 'Siamese_model.pt'\n"]}]},{"cell_type":"code","source":["### =========== MATCHING IMAGE & NUMBER IDENTIFICATION\n","\n","#set the seed for reprod\n","SEED = 42\n","# Python random seed\n","random.seed(SEED)\n","# NumPy random seed\n","np.random.seed(SEED)\n","# PyTorch seeds\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","# Ensure deterministic behavior in cuDNN (slower but repeatable)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","#show if it is done\n","print(f\"✅ Reproducibility set: all seeds fixed to {SEED}\")\n","\n","#prompt to upload an image file (showing a 1-digit number)\n","uploaded = files.upload()\n","\n","#load and preprocess the uploaded image\n","#Note. MNIT imgs are black back-white digits -> need to transform the uploaded img\n","#into this layout\n","for filename in uploaded.keys():\n","  #load uploaded img from memory\n","    img = Image.open(io.BytesIO(uploaded[filename]))  # load image\n","    #convert to grayscale (MNIST files are grayscale)\n","    gray_img = img.convert(\"L\")  # single channel\n","    #convert to numpy array\n","    gray_np = np.array(gray_img)\n","    #if img's back is bright (white) invert colors\n","    if np.mean(gray_np) > 127:\n","        gray_img = ImageOps.invert(gray_img)\n","    #resize to MNIST standards (28x28 pixels)\n","    gray_img = gray_img.resize((28, 28))\n","    #convert to tensor and add batch dimension (for model input)\n","    user_img = transforms.ToTensor()(gray_img).unsqueeze(0).cuda()\n","\n","    #compute embedding of the uploaded image thanks to the CNN\n","    tun_model.eval() #set model in evaluation mode\n","    #no gradient computation\n","    with torch.no_grad():\n","        user_embed = tun_model.forward_once(user_img)\n","\n","    #arrange a pool of test imgs for comparison\n","    #compare against entire test set\n","    n_samples = len(data_tt)\n","    #store img tensors\n","    test_imgs = []\n","    #store true digits labels\n","    test_labels = []\n","    #store embeddings\n","    test_embeds = []\n","\n","    #loop over all test imgs\n","    for i in range(n_samples):\n","      #extract test img and its label\n","        test_img, test_label = data_tt[i]\n","        #apply same pre-processing (re-size + tensor conversion)\n","        test_img_proc = transforms.ToTensor()(test_img.resize((28, 28)))\n","        #save pre-processed img\n","        test_imgs.append(test_img_proc.squeeze().numpy())\n","        #and its label\n","        test_labels.append(test_label)\n","        #compute embedding for the test img\n","        with torch.no_grad():\n","            test_embeds.append(tun_model.forward_once(test_img_proc.unsqueeze(0).cuda()))\n","\n","    #compute euclidean distance between uploaded img and all test imgs\n","    distances = [F.pairwise_distance(user_embed, e).item() for e in test_embeds]\n","\n","    #Show the most similar img (smaller distance)\n","    #Note. tweak this to show the n-top similar images\n","    top_idx = np.argsort(distances)[:1]\n","    top_matches = [test_imgs[i] for i in top_idx]\n","    top_labels = [test_labels[i] for i in top_idx]\n","    #convert distance into similarity score (higher = more similar)\n","    top_scores = [np.exp(-distances[i]) for i in top_idx]\n","\n","    #display the upload img + its closest match\n","    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n","    #show uploaded img\n","    axes[0].imshow(user_img.cpu().squeeze().numpy(), cmap=\"gray\")\n","    axes[0].set_title(\"🔹 Uploaded\")\n","    axes[0].axis(\"off\")\n","    #show the most similar test img\n","    for j in range(1):\n","        axes[j+1].imshow(top_matches[j], cmap=\"gray\")\n","        axes[j+1].set_title(f\"#{j+1} | {top_labels[j]}\\nSim: {top_scores[j]:.3f}\")\n","        axes[j+1].axis(\"off\")\n","    #show plot\n","    plt.tight_layout()\n","    plt.show()\n","\n","    #print the output\n","    print(\"=======================================\")\n","    print(f\"The uploaded img is a {top_labels[j]}\")\n","    print(\"=======================================\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"PDyQIGl6zsJS","executionInfo":{"status":"ok","timestamp":1760715875238,"user_tz":-120,"elapsed":15443,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}},"outputId":"2a567a31-9e58-4e2f-db27-17652dccf0c4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Reproducibility set: all seeds fixed to 42\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-bcc71f4c-f305-4b5e-a93a-0d9bf03ee3d2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-bcc71f4c-f305-4b5e-a93a-0d9bf03ee3d2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Unknown.jpeg to Unknown.jpeg\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3760745053.py:93: UserWarning: Glyph 128313 (\\N{SMALL BLUE DIAMOND}) missing from font(s) DejaVu Sans.\n","  plt.tight_layout()\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x300 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=======================================\n","The uploaded img is a 6\n","=======================================\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/Lulloooo/SiameseNet-NumRecognition.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_t5GoXl2lzo","executionInfo":{"status":"ok","timestamp":1760716332201,"user_tz":-120,"elapsed":509,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}},"outputId":"227370d6-9fab-482d-b1d8-d12935c84da4"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'SiameseNet-NumRecognition'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (3/3), done.\n"]}]},{"cell_type":"code","source":["# Remove any previous local copy and clone a fresh one\n","!rm -rf SiameseNet-NumRecognition\n","!git clone https://github.com/Lulloooo/SiameseNet-NumRecognition.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8G-i4YK545i","executionInfo":{"status":"ok","timestamp":1760716963618,"user_tz":-120,"elapsed":620,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}},"outputId":"d2689755-baff-413c-8058-5e903f07867e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'SiameseNet-NumRecognition'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (3/3), done.\n"]}]},{"cell_type":"code","source":["REPO = \"SiameseNet-NumRecognition\"\n","# Paths to your Colab notebooks and model files\n","NOTEBOOK1=\"/content/drive/MyDrive/Colab_Notebooks/Siamese_Train&Evaluation.ipynb\"\n","NOTEBOOK2=\"/content/drive/MyDrive/Colab_Notebooks/Siamese_predict.ipynb\"\n","MODEL1=\"/content/drive/MyDrive/tuning_siamese_model.pt\"\n","MODEL2=\"/content/drive/MyDrive/Siamese_model.pt\"\n","\n","# Copy them into the cloned repo\n","!cp \"$NOTEBOOK1\" \"$REPO/\"\n","!cp \"$NOTEBOOK2\" \"$REPO/\"\n","!cp \"$MODEL1\" \"$REPO/\"\n","!cp \"$MODEL2\" \"$REPO/\"\n"],"metadata":{"id":"9IXetITM6PW-","executionInfo":{"status":"ok","timestamp":1760717007907,"user_tz":-120,"elapsed":2856,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Go into the repo directory\n","%cd $REPO\n","\n","# Configure git (only once per Colab session)\n","!git config user.email \"lucagabri98@live.com\"\n","!git config user.name \"Lulloooo\"\n","\n","# Add and commit the new files\n","!git add .\n","!git commit -m \"Added Siamese training and prediction notebooks + model files\"\n","\n","# Push to GitHub\n","!git push origin main\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RPWP3-l6mMO","executionInfo":{"status":"ok","timestamp":1760717101417,"user_tz":-120,"elapsed":1834,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}},"outputId":"75f36641-2faf-4bb2-aa2b-e2d8b0821c67"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/SiameseNet-NumRecognition\n","[main 3714aef] Added Siamese training and prediction notebooks + model files\n"," 4 files changed, 2 insertions(+)\n"," create mode 100644 Siamese_Train&Evaluation.ipynb\n"," create mode 100644 Siamese_model.pt\n"," create mode 100644 Siamese_predict.ipynb\n"," create mode 100644 tuning_siamese_model.pt\n","fatal: could not read Username for 'https://github.com': No such device or address\n"]}]},{"cell_type":"code","source":["!zip -r /content/SiameseNet_Files.zip \\\n","    /content/drive/MyDrive/Colab_Notebooks/Siamese_TrainAndEvaluation.ipynb \\\n","    /content/drive/MyDrive/Colab_Notebooks/Siamese_predict.ipynb \\\n","    /content/drive/MyDrive/Siamese_model.pt \\\n","    /content/drive/MyDrive/tuning_siamese_model.pt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFu3IJJ8-spp","executionInfo":{"status":"ok","timestamp":1760718356040,"user_tz":-120,"elapsed":2643,"user":{"displayName":"lucagabri98@live.it","userId":"11089164872783927144"}},"outputId":"9b14dad0-b3f5-4192-c947-94b20b6a5f0a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\tzip warning: name not matched: /content/drive/MyDrive/Colab_Notebooks/Siamese_TrainAndEvaluation.ipynb\n","  adding: content/drive/MyDrive/Colab_Notebooks/Siamese_predict.ipynb (deflated 58%)\n","  adding: content/drive/MyDrive/Siamese_model.pt (deflated 8%)\n","  adding: content/drive/MyDrive/tuning_siamese_model.pt (deflated 9%)\n"]}]}]}